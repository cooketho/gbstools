import collections
import math
import vcf
import pysam
from numpy import median
from vcf.model import make_calldata_tuple
import em

try:
    from collections import Counter
except ImportError:
    from counter import Counter

""" Confusion matrix for Illumina sequencers. Keyed by [actual base][read base]
(see DePristo et al 2010. """
CONFUSION_MATRIX = {'A':{'A':None, 'C':0.577, 'G':0.171, 'T':0.252},
                    'C':{'A':0.349, 'C':None, 'G':0.113, 'T':0.539},
                    'G':{'A':0.319, 'C':0.051, 'G':None, 'T':0.630},
                    'T':{'A':0.458, 'C':0.221, 'G':0.320, 'T':None}}

"""INFO fields to be added to the vcf header by GBStools."""
_Info = collections.namedtuple('Info', ['id', 'num', 'type', 'desc'])
INFO = (_Info('DLR', None, 'Float', 'Dropout likelihood ratio (GBStools)'),
        _Info('DFreq', None, 'Float', 'Dropout allele frequency estimated by EM (GBStools)'),
        _Info('AFH1', None, 'Float', 'Allele frequency estimated by EM (GBStools)'),
        _Info('AFH0', None, 'Float', 'Null hypothesis allele frequency estimated by EM (GBStools)'),
        _Info('LambdaH1', None, 'Float', 'Normalized mean coverage estimated by EM (GBStools)'),
        _Info('LambdaH0', None, 'Float', 'Null hypothesis normalized mean coverage estimated by EM (GBStools)'),
        _Info('DigestH1', None, 'Float', 'Digest failure rate estimated by EM (GBStools)'),
        _Info('DigestH0', None, 'Float', 'Null hypothesis digest failure rate estimated by EM (GBStools)'),
        _Info('IterationH1', None, 'Integer', 'Number of null hypothesis EM iterations (GBStools)'),
        _Info('IterationH0', None, 'Integer', 'Number of alt hypothesis EM iterations (GBStools)'),
        _Info('FailH1', None, 'Flag', 'EM failure flag'),
        _Info('FailH0', None, 'Flag', 'Null hypothesis EM failure flag'),
        _Info('FwdRS', None, 'String', 'Recognition sites for forward reads'),
        _Info('RevRS', None, 'String', 'Recognition sites for reverse reads'),
        _Info('InsMed', None, 'Float', 'Insert size median'),
        _Info('InsMAD', None, 'Float', 'Insert size MAD'))

"""FORMAT fields to be added to the vcf header by GBStools."""
_Format = collections.namedtuple('Format', ['id', 'num', 'type', 'desc'])
FORMAT = (_Format('DC', None, 'Integer', 'Dropout allele count'),
          _Format('INS', None, 'Integer', 'Median insert size'),
          _Format('NF', None, 'Integer', 'Normalization factor for sample DP'))

class Reader():
    """Reader for a VCF file, an iterator returning ``_Marker`` objects."""
    def __init__(self, filename, bamlist=None, norm=None, disp=2.5, snpmode=True):
                 
        """Create a new Reader for a VCF file containing GBS data.

           To get marker data from indexed bam files, use bamlist=mybamlist 
           where mybamlist is in the format of name/file per line.

           To normalize the read coverages according to the relative
           numbers of reads across samples, use norm=<mynormfile>,
           generated by normfactors.r

           To set the coverage dispersion index use the disp option
        """
        # Make a generator for vcf records.
        self._reader = vcf.Reader(filename=filename)
        self.reader = (record for record in self._reader)
        self.filename = filename
        # Make a list of Samfile objects for fetching read info.
        if bamlist:
            self.alignments = self.parse_bamlist(bamlist)
        else:
            self.alignments = None
        if norm:
            self.normfactors = self.parse_norm(norm)
        else:
            self.normfactors = None
        self.disp = disp
        # Should SNP+DP mode or DP-only mode be used?
        self.snpmode = snpmode

    def parse_norm(self, norm):
        '''Parse the normalization factors file and store data in a hash.'''
        normfactors = {}    # Hash of hashes keyed by insert; sample.
        norm = open(norm, 'r')
        # The first two header fields define the bin limits; the rest are sample names.
        header = norm.readline()
        header = header.strip().split()
        for line in norm:
            line = line.strip()
            fields = line.split()
            lower = int(fields[0])    # Lower bin limit for normalization factor.
            upper = int(fields[1])    # Upper bin limit.
            # Use range on each pair of bin limits to make hash keys.
            for i in range(lower, upper + 1):
                normfactors[i] = {}
                for j in range(2, len(header)):
                    normfactors[i][header[j]] = float(fields[j])
        return(normfactors)
    
    def parse_bamlist(self, bamlist):
        '''Parse the bamlist file and return a list of Samfile objects.'''
        bamlist = open(bamlist, 'r')
        alignments = {}
        for line in bamlist:
            line = line.strip()
            sample, bam = line.split()
            alignments[sample] = _Samfile(bam, sample)
        return(alignments)

    def __iter__(self):
        return self
   
    def next(self):
        # Extract SNP info from the vcf file.
        vcf_record = self.reader.next()
        chrom = vcf_record.CHROM
        pos = vcf_record.POS - 1
        ref = vcf_record.REF
        alt = vcf_record.ALT
        # Generate a list of ''CallData'' objects in the same order as in vcf.
        calls = []
        for sample in vcf_record.samples:
            if self.alignments:
                # Get read data directly from bam file.
                alignment = self.alignments[sample.sample]
                call = alignment.pileup(chrom, pos, ref, alt[0])
                # Look up the normalization factor based on insert size.
                try:
                    call.NF = self.normfactors[sample.sample][call.INS]
                except:
                    call.NF = None
                calls.append(call)
            else:
                # Get read data from vcf if bam files were not provided.
                keys = sample.data._fields
                vals = list(iter(sample.data))
                data = dict(zip(keys, vals))
                calls.append(CallData(sample.sample, **data))
        # If DP-only mode is being used, set PL to None.
        if not self.snpmode:
            for call in calls:
                call.PL = None
        # Create a dictionary of vcf INFO field tags.
        info = {}
        # Calculate insert size median and MAD.
        inserts = [ins for sample in calls for ins in sample.inserts]
        try:
            ins_med = median(inserts)
            info['InsMed'] = ins_med
            info['InsMAD'] = median([abs(ins - ins_med) for ins in inserts])
        except:
            info['InsMed'] = None
            info['InsMAD'] = None
        # Get fwd and reverse enzymes.
        fwd_rs_list = [rs for sample in calls for rs in sample.fwd_rs]
        rev_rs_list = [rs for sample in calls for rs in sample.rev_rs]
        try:
            fwd_rs_tally = Counter(fwd_rs_list).most_common(1)
            fwd_rs, count = fwd_rs_tally.pop()
            fwd_rs = fwd_rs.replace(';', ',')
            info['FwdRS'] = "%s,%i" % (fwd_rs, count)
        except:
            pass
        try:
            rev_rs_tally = Counter(rev_rs_list).most_common(1)
            rev_rs, count = rev_rs_tally.pop()
            rev_rs = rev_rs.replace(';', ',')
            info['RevRS'] = "%s,%i" % (rev_rs, count)
        except:
            pass
        # Generate ''Marker'' object.
        marker = Marker(rec=vcf_record, calls=calls, disp=self.disp, info=info)
        return(marker)

    def fetch(self, chrom, start, end=None):
        self.reader = self._reader.fetch(chrom, start, end)
        return self


class Writer():
    """Output GBS marker data in VCF format."""
    def __init__(self, outstream, template, lineterminator='\n'):
        filename = template.filename
        disp = template.disp
        self.template = vcf.Reader(filename=filename)
        for info in INFO:
            self.template.infos[info.id] = info
        for format in FORMAT:
            self.template.formats[format.id] = format
        analysis = "input_file=%s dispersion=%f" % (filename, disp)
        self.template.metadata['GBStools'] = [analysis]
        self.writer = vcf.Writer(outstream, self.template, lineterminator)
        
    def write_record(self, marker):
        '''Write the marker data to outstream.'''
        # Update the vcf INFO field.
        for info_id, val in marker.info.items():
            if isinstance(val, float):
                marker.record.add_info(info_id, round(val, 3))
            elif val is not None:
                marker.record.add_info(info_id, val)
        # Hash normalization factors and insert sizes, keyed by sample name.
        nf = {}
        ins = {}
        for call in marker.calls:
            nf[call.sample] = call.NF
            ins[call.sample] = call.INS
        # Update the vcf FORMAT field.
        if 'NF' not in marker.record.FORMAT:
            marker.record.add_format('NF')
        if 'INS' not in marker.record.FORMAT:
            marker.record.add_format('INS')
        # Update the vcf sample data.
        for sample in marker.record.samples:
            ids = list(sample.data._fields)
            vals = list(iter(sample.data))
            if 'NF' not in ids:
                ids.append('NF')
                try:
                    vals.append(round(nf[sample.sample], 3))
                except:
                    vals.append(None)
            if 'INS' not in ids:
                ids.append('INS')
                try:
                    vals.append(int(ins[sample.sample]))
                except:
                    vals.append(None)
            new_cls = make_calldata_tuple(ids)
            sample.data = new_cls._make(vals)
        # Write record to outstream.
        self.writer.write_record(marker.record)
        return(None)


class Marker():
    """Store data from a single GBS SNP marker and call EM functions."""
    def __init__(self, rec, calls, disp, info):
        self.record = rec
        self.calls = calls
        self.disp = disp
        self.info = info
        self.param = []
        self.null_param = []
        self.lik_ratio = None
        # Initial dropout frequency.
        dfreq = 0.01
        # Bool indicating allele data is present.        
        plflag = len([call.PL for call in calls if call.PL]) > 0
        if plflag:
            try:
                af = min(0.9999, self.record.INFO['AF'][0])
            except:
                af = 0.01
            phi0 = [(1 - dfreq) * (1 - af), (1 - dfreq) * af, dfreq]
            phi0_null = [1 - af, af, 0]
        else:
            phi0 = [0, 0, dfreq]
            phi0_null = [0, 0, 0]
        dp = sum([call.DP for call in calls])
        missing = sum([call.DP == 0 for call in calls])
        if dp > 0:
            lambda0 = float(dp) / (len(calls) - missing)
            delta0 = max(float(missing) / len(calls), 0.01)
            fail = False
        else:
            lambda0 = None
            delta0 = None
            fail = True
        self.param.append({'phi':phi0, 
                           'lambda':lambda0,
                           'delta':delta0,
                           'converged':False, 
                           'fail':fail,
                           'loglik':None})
        self.null_param.append({'phi':phi0_null, 
                                'lambda':lambda0,
                                'delta':delta0,
                                'converged':False, 
                                'fail':fail, 
                                'loglik':None})
        
    def update_param(self, param, phi_tol=0.001, lamb_tol=0.1):
        '''Update the parameter estimates by EM (see GBStools notes).'''
        try:
            param_new = em.update(param[-1], self.calls, self.disp)
            phi_diff = (max(abs(param_new['phi'][1] - param[-1]['phi'][1]),
                            abs(param_new['phi'][2] - param[-1]['phi'][2])))
            lamb_diff = abs(param_new['lambda'] - param[-1]['lambda'])
            if phi_diff <= phi_tol and lamb_diff <= lamb_tol:
                param_new['converged'] = True
        except:
            param_new = param[-1]
            param_new['fail'] = True
        param.append(param_new)
        return None

    def likelihood_ratio(self):
        '''Null hypothesis: phi[2] == 0. Alt hypothesis: phi[2] > 0.'''
        try:
            lr = -2.0 * (self.null_param[-1]['loglik'] - self.param[-1]['loglik'])
        except:
            lr = None
        return(lr)
    
    def update_info(self):
        '''Update the INFO field for the output vcf.'''
        self.info['DLR'] = self.lik_ratio
        self.info['DFreq'] = self.param[-1]['phi'][2]
        self.info['AFH1'] = self.param[-1]['phi'][1]
        self.info['AFH0'] = self.null_param[-1]['phi'][1]
        self.info['DigestH1'] = self.param[-1]['delta']
        self.info['DigestH0'] = self.null_param[-1]['delta']
        self.info['IterationH1'] = len(self.param)
        self.info['IterationH0'] = len(self.null_param)
        self.info['EMFailH1'] = self.param[-1]['fail']
        self.info['EMFailH0'] = self.null_param[-1]['fail']
        return(None)


class _Samfile():
    """ Class for generating ''Pileup'' objects from pysam ''Samfile'' objects. """
    def __init__(self, bam, sample):
        self.sample = sample
        self.bam = pysam.Samfile(bam, 'rb')

    def pileup(self, chrom, pos, ref, alt):
        """ Return ''CallData'' object containing DP, PL etc."""
        # Generate pysam ''Pileup'' object.
        pileup = self.bam.pileup(chrom, pos, pos + 1, truncate=True)
        # Extract data from pileup.
        data = _PileupData(pileup, ref, alt)
        call_data = CallData(self.sample, **data.data)
        return(call_data)


class _PileupData():
    """ Class for extracting read information from single loci in ''Pileup'' objects. """
    def __init__(self, pileup, ref, alt, maxcovg=250, offset=33):
        # The pysam ''PileupProxy'' for this sample.
        self.pileup = pileup
        self.data = {}
        try:
            # Get the pysam ''PileupColumn''.
            pileupcol = self.pileup.next()
            # Get list of pysam ''PileupRead'' objects.
            reads = [read for read in pileupcol.pileups if read.alignment.mapq > 0]
            self.reads = reads[:maxcovg]
            self.data['DP'] = len(self.reads)
        except:
            self.reads = []
            self.data['DP'] = 0
        # Calculate PL.
        self.ref = ref
        try:
            self.alt = str(alt)
            self.data['PL'] = self.calculate_pl(offset)
        except:
            self.alt = None
            self.data['PL'] = None
        # Extract info from read tags.
        inserts, fwd_rs, rev_rs = self.extract_tags()
        self.data['inserts'] = inserts
        if inserts:
            self.data['INS'] = median(inserts)
        else:
            self.data['INS'] = None
        self.data['fwd_rs'] = fwd_rs
        self.data['rev_rs'] = rev_rs

    def extract_tags(self):
        '''Extract insert and enzyme info from read tags (see annotate_bam.py).'''
        tags = [dict(read.alignment.tags) for read in self.reads]
        inserts = [tag['Z0'] for tag in tags if 'Z0' in tag]
        fwd_rs = [tag['Z2'] for tag in tags if 'Z2' in tag]
        rev_rs = [tag['Z4'] for tag in tags if 'Z4' in tag]
        return(inserts, fwd_rs, rev_rs)

    def calculate_insert_med(self):
        '''Calculate insert size before read trimming.'''
        if self.inserts:
            ins_med = median(self.inserts)
            ins_mad = median([abs(ins_med - insert) for insert in self.inserts])
        else:
            ins_med = None
            ins_mad = None
        return(ins_med, ins_mad)
        

    def calculate_pl(self, offset):
        '''Calculate genotype likelihoods and allele depth from reads.'''
        homref_lik = 0
        het_lik = 0
        homnonref_lik = 0
        for read in self.reads:
            base = read.alignment.seq[read.qpos]
            # Calculate Pr(ref is true | base is miscalled)
            pr_ref = CONFUSION_MATRIX[self.ref][self.alt]
            # Calculate Pr(alt is true | base is miscalled)
            pr_alt = CONFUSION_MATRIX[self.alt][self.ref]
            # Calculate the base call error rate, epsilon.
            qual = read.alignment.qual[read.qpos]
            epsilon = 10**(-ord(qual - offset) / 10.0)

            if base == self.ref:
                homref_lik += math.log(1 - epsilon, 10)
                het_lik += math.log((1 - epsilon * (1 - pr_alt)) / 2.0, 10)
                homref_lik += math.log(epsilon * pr_alt, 10)

            elif base == self.alt:
                homref_lik += math.log(epsilon * pr_ref, 10)
                het_lik += math.log((1 - epsilon * (1 - pr_ref)) / 2.0, 10)
                homref_lik += math.log(1 - epsilon, 10)

        lik = (homref_lik, het_lik, homnonref_lik)
        # Normalized the likelihoods.
        maxlik = max(lik)
        pl = [-10 * (l - maxlik) for l in lik]
        return(pl)


class CallData():
    """Class for storing data for an individual sample."""
    def __init__(self, sample, **kwargs):
        self.sample = sample
        dp = kwargs.pop('DP', 0)
        if dp is None:
            self.DP = 0
        else:
            self.DP = dp
        pl = kwargs.pop('PL', None)
        if pl:
            self.PL = [int(i) for i in pl]
        else:
            self.PL = None
        self.NF = kwargs.pop('NF', 1.0)
        self.inserts = kwargs.pop('inserts', [])
        self.INS = kwargs.pop('INS', None)
        self.fwd_rs = kwargs.pop('fwd_rs', [])
        self.rev_rs = kwargs.pop('rev_rs', [])
